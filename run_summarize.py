import argparse
import os
import torch
from transformers import PegasusForConditionalGeneration, PegasusTokenizer, pipeline

def detect_latest_checkpoint(parent_dir: str) -> str:
    """
    T·ª± ƒë·ªông ch·ªçn checkpoint c√≥ global_step l·ªõn nh·∫•t trong parent_dir.
    V√≠ d·ª•: FINAL/checkpoint-3000, FINAL/checkpoint-3682 -> tr·∫£ v·ªÅ checkpoint-3682
    """
    if not os.path.isdir(parent_dir):
        raise ValueError(f"Parent dir kh√¥ng t·ªìn t·∫°i: {parent_dir}")

    cand = []
    for name in os.listdir(parent_dir):
        full = os.path.join(parent_dir, name)
        if os.path.isdir(full) and name.startswith("checkpoint-"):
            try:
                step = int(name.split("-")[-1])
                cand.append((step, full))
            except Exception:
                pass
    if not cand:
        raise ValueError(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c checkpoint-* trong {parent_dir}")
    cand.sort(key=lambda x: x[0], reverse=True)
    return cand[0][1]

def load_model_tokenizer(model_dir: str):
    """
    Load tokenizer + model t·ª´ th∆∞ m·ª•c checkpoint ƒë√£ fine-tune.
    N·∫øu thi·∫øu file tokenizer trong checkpoint, c√≥ th·ªÉ ƒë·ªïi sang base tokenizer:
      base_tok = 'google/pegasus-samsum'  ho·∫∑c  'google/pegasus-large'
    """
    print(f"üîπ Loading from: {model_dir}")
    tokenizer = PegasusTokenizer.from_pretrained(model_dir)
    model = PegasusForConditionalGeneration.from_pretrained(model_dir)
    return model, tokenizer

def build_pipeline(model, tokenizer, device_preference: str):
    # Ch·ªçn thi·∫øt b·ªã
    if device_preference == "cpu":
        device = -1
    else:
        device = 0 if torch.cuda.is_available() else -1
    print("üîπ Device:", "GPU" if device == 0 else "CPU")

    summarizer = pipeline(
        "summarization",
        model=model,
        tokenizer=tokenizer,
        device=device
    )
    return summarizer

def summarize_texts(summarizer, texts, max_len, min_len, num_beams, do_sample):
    outputs = []
    for t in texts:
        t = t.strip()
        if not t:
            continue
        out = summarizer(
            t,
            max_length=max_len,
            min_length=min_len,
            num_beams=num_beams,
            do_sample=do_sample
        )[0]["summary_text"]
        outputs.append((t, out))
    return outputs

def main():
    parser = argparse.ArgumentParser(description="Run Pegasus summarization from fine-tuned checkpoint.")
    parser.add_argument("--parent_dir", type=str, default="FINAL",
                        help="Th∆∞ m·ª•c cha ch·ª©a c√°c checkpoint-xxxx (local) ho·∫∑c ch√≠nh l√† checkpoint folder.")
    parser.add_argument("--model_dir", type=str, default=None,
                        help="ƒê∆∞·ªùng d·∫´n tr·ª±c ti·∫øp t·ªõi checkpoint. N·∫øu b·ªè tr·ªëng, script t·ª± ch·ªçn checkpoint m·ªõi nh·∫•t trong parent_dir.")
    parser.add_argument("--text", type=str, default=None,
                        help="M·ªôt ƒëo·∫°n h·ªôi tho·∫°i ƒë·ªÉ t√≥m t·∫Øt (∆∞u ti√™n h∆°n --file).")
    parser.add_argument("--file", type=str, default=None,
                        help="ƒê∆∞·ªùng d·∫´n file .txt (m·ªói d√≤ng l√† 1 input c·∫ßn t√≥m t·∫Øt).")
    parser.add_argument("--max_len", type=int, default=60)
    parser.add_argument("--min_len", type=int, default=10)
    parser.add_argument("--num_beams", type=int, default=4)
    parser.add_argument("--do_sample", action="store_true", help="B·∫≠t sampling (m·∫∑c ƒë·ªãnh t·∫Øt).")
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cpu"], help="Ch·∫°y GPU n·∫øu c√≥ (auto) ho·∫∑c b·∫Øt bu·ªôc CPU.")

    args = parser.parse_args()

    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n checkpoint
    if args.model_dir is None:
        # N·∫øu parent_dir ch√≠nh l√† m·ªôt checkpoint h·ª£p l·ªá th√¨ d√πng lu√¥n
        if os.path.isdir(args.parent_dir) and os.path.isfile(os.path.join(args.parent_dir, "config.json")):
            model_dir = args.parent_dir
        else:
            # Ng∆∞·ª£c l·∫°i, t√¨m checkpoint-* m·ªõi nh·∫•t trong parent_dir
            model_dir = detect_latest_checkpoint(args.parent_dir)
    else:
        model_dir = args.model_dir

    # Load model + tokenizer
    model, tokenizer = load_model_tokenizer(model_dir)
    summarizer = build_pipeline(model, tokenizer, args.device)

    # Chu·∫©n b·ªã input
    texts = []
    if args.text:
        texts = [args.text]
    elif args.file:
        if not os.path.isfile(args.file):
            raise ValueError(f"Kh√¥ng t√¨m th·∫•y file: {args.file}")
        with open(args.file, "r", encoding="utf-8") as f:
            texts = [line.strip() for line in f.readlines()]
    else:
        # v√≠ d·ª• m·∫∑c ƒë·ªãnh
        texts = ["""A: Hey, did you book the venue for Friday?
B: Not yet, I'm comparing prices.
A: We need it by tonight.
B: Okay, I'll finalize the booking."""]

    # Ch·∫°y t√≥m t·∫Øt
    results = summarize_texts(
        summarizer, texts,
        max_len=args.max_len, min_len=args.min_len,
        num_beams=args.num_beams, do_sample=args.do_sample
    )

    # In k·∫øt qu·∫£
    print("\n===== RESULTS =====")
    for i, (src, hyp) in enumerate(results, 1):
        print(f"\n[# {i}] SOURCE:\n{src}\n---\nSUMMARY:\n{hyp}")

if __name__ == "__main__":
    main()
