{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ed60ae5c","cell_type":"markdown","source":"## 0) CÃ i Ä‘áº·t & TÆ°Æ¡ng thÃ­ch Kaggle","metadata":{}},{"id":"5b6adf16","cell_type":"code","source":"!pip -q install \"pyarrow<17,>=10\" \"datasets==2.20.0\" \"transformers==4.44.2\"                 \"evaluate==0.4.2\" \"accelerate>=0.31.0\" \"sentencepiece>=0.1.99\"                 \"rouge-score==0.1.2\" \"nltk>=3.8.1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:55:55.055862Z","iopub.execute_input":"2025-10-04T08:55:55.056482Z","iopub.status.idle":"2025-10-04T08:57:31.530218Z","shell.execute_reply.started":"2025-10-04T08:55:55.056445Z","shell.execute_reply":"2025-10-04T08:57:31.529546Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.5.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"744403de","cell_type":"markdown","source":"## 1) Import & Thiáº¿t láº­p","metadata":{}},{"id":"dd2d741e","cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport nltk\nnltk.download(\"punkt\")\n\nimport evaluate\nfrom datasets import load_dataset\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\n\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:57:31.531604Z","iopub.execute_input":"2025-10-04T08:57:31.531858Z","iopub.status.idle":"2025-10-04T08:57:54.887380Z","shell.execute_reply.started":"2025-10-04T08:57:31.531835Z","shell.execute_reply":"2025-10-04T08:57:54.886801Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n2025-10-04 08:57:42.359029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759568262.548193      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759568262.608666      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Torch: 2.6.0+cu124\nCUDA available: True\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":2},{"id":"dc605d67","cell_type":"markdown","source":"## 2) Táº£i dá»¯ liá»‡u SAMSum","metadata":{}},{"id":"41007078","cell_type":"code","source":"dataset = load_dataset(\"knkarthick/samsum\")\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:57:54.888080Z","iopub.execute_input":"2025-10-04T08:57:54.888703Z","iopub.status.idle":"2025-10-04T08:58:03.231233Z","shell.execute_reply.started":"2025-10-04T08:57:54.888683Z","shell.execute_reply":"2025-10-04T08:58:03.230545Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e40d609b9f4f1d88653f98c4b78945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/9.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1afa38501458437b891144e9f51ed3b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/504k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"421050344ea149eab7395cd6587f08a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/522k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e8c2e1cd834de293f25c1e28b56ccd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14731 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef54fd36c4784383a9a848d47243fcbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc23239eaf6c435ba933d7a0fd0e9766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5573a9a3a049d489f11f87c54fa8ac"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14731\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n})"},"metadata":{}}],"execution_count":3},{"id":"974fbfff","cell_type":"markdown","source":"## 3) Cáº¥u hÃ¬nh Tokenizer/Model","metadata":{}},{"id":"fd76baa4","cell_type":"code","source":"model_name = \"google/pegasus-large\"  # hoáº·c \"google/pegasus-samsum\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.to(device)\n\nmax_input_length  = 512\nmax_target_length = 128\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:58:03.232956Z","iopub.execute_input":"2025-10-04T08:58:03.233169Z","iopub.status.idle":"2025-10-04T08:58:25.954448Z","shell.execute_reply.started":"2025-10-04T08:58:03.233152Z","shell.execute_reply":"2025-10-04T08:58:25.953906Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47acdef6fde4d02bce322dbcc89f10a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bebd912ffa24d3bb6f528c32af447f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a20132147434d39859d2c23429be1f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a0cf34ae194e6e9b78f2d2c265334b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49255282e4ac43c5853ac8c1c94646b5"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb8044c9a65b4069bd702c687ca4e552"}},"metadata":{}}],"execution_count":4},{"id":"6696f1bb","cell_type":"markdown","source":"## 4) Tiá»n xá»­ lÃ½","metadata":{}},{"id":"7557dee2","cell_type":"code","source":"def preprocess_function(batch):\n    inputs = batch[\"dialogue\"]\n    targets = batch[\"summary\"]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=max_target_length,\n            truncation=True\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprocessed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\nprocessed_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:58:25.955188Z","iopub.execute_input":"2025-10-04T08:58:25.955442Z","iopub.status.idle":"2025-10-04T08:58:31.517720Z","shell.execute_reply.started":"2025-10-04T08:58:25.955416Z","shell.execute_reply":"2025-10-04T08:58:31.517112Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14731 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6012c9950b6e45348e523664c5efc81e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5064fb539ce046d294e2941708f61b4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2ed156684045c9ac2c5b4bf83db191"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 14731\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 818\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 819\n    })\n})"},"metadata":{}}],"execution_count":5},{"id":"a2e9c5a5","cell_type":"markdown","source":"## 5) Data Collator & ROUGE metric","metadata":{}},{"id":"fe93a61f","cell_type":"code","source":"from rouge_score import rouge_scorer, scoring\nimport numpy as np\n\n# Táº¡o scorer local (khÃ´ng cáº§n máº¡ng, khÃ´ng bá»‹ conflict tÃªn thÆ° má»¥c)\n_scorer = rouge_scorer.RougeScorer(\n    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n    use_stemmer=True\n)\n\ndef postprocess_text(preds, labels):\n    preds = [p.strip() for p in preds]\n    labels = [l.strip() for l in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    # Decode\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # TÃ­nh ROUGE báº±ng bootstrap aggregator\n    aggregator = scoring.BootstrapAggregator()\n    for pred, ref in zip(decoded_preds, decoded_labels):\n        aggregator.add_scores(_scorer.score(ref, pred))\n    result = aggregator.aggregate()\n\n    # Láº¥y F1 (mid.fmeasure) vÃ  chuyá»ƒn vá» %\n    out = {\n        \"rouge1\": round(result[\"rouge1\"].mid.fmeasure * 100, 2),\n        \"rouge2\": round(result[\"rouge2\"].mid.fmeasure * 100, 2),\n        \"rougeL\": round(result[\"rougeL\"].mid.fmeasure * 100, 2),\n        \"rougeLsum\": round(result[\"rougeLsum\"].mid.fmeasure * 100, 2),\n    }\n\n    # Äá»™ dÃ i trung bÃ¬nh cá»§a chuá»—i sinh ra\n    prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n    out[\"gen_len\"] = float(np.mean(prediction_lens))\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:58:31.518528Z","iopub.execute_input":"2025-10-04T08:58:31.518795Z","iopub.status.idle":"2025-10-04T08:58:31.530306Z","shell.execute_reply.started":"2025-10-04T08:58:31.518777Z","shell.execute_reply":"2025-10-04T08:58:31.529621Z"}},"outputs":[],"execution_count":6},{"id":"b231b125","cell_type":"markdown","source":"## 6) Thiáº¿t láº­p tham sá»‘ huáº¥n luyá»‡n","metadata":{}},{"id":"b1e83def","cell_type":"code","source":"output_dir = \"pegasus-samsum-finetuned\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    evaluation_strategy=\"steps\",\n    eval_steps=1000,\n    save_steps=1000,\n    logging_steps=200,\n    save_total_limit=2,\n    learning_rate=5e-5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,\n    predict_with_generate=True,\n    generation_max_length=max_target_length,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=2,\n    report_to=\"none\",\n    metric_for_best_model=\"rougeLsum\",\n    load_best_model_at_end=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:01:20.471886Z","iopub.execute_input":"2025-10-04T09:01:20.472183Z","iopub.status.idle":"2025-10-04T09:01:20.499798Z","shell.execute_reply.started":"2025-10-04T09:01:20.472161Z","shell.execute_reply":"2025-10-04T09:01:20.498951Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"id":"de7a6000","cell_type":"markdown","source":"## 7) Huáº¥n luyá»‡n","metadata":{}},{"id":"3bcec6e7","cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_datasets[\"train\"],\n    eval_dataset=processed_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n    compute_metrics=compute_metrics,\n)\n\ntrain_result = trainer.train()\ntrain_result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:02:44.035367Z","iopub.execute_input":"2025-10-04T09:02:44.036278Z","iopub.status.idle":"2025-10-04T10:46:41.334829Z","shell.execute_reply.started":"2025-10-04T09:02:44.036243Z","shell.execute_reply":"2025-10-04T10:46:41.334010Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3682' max='3682' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3682/3682 1:43:54, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>1.488700</td>\n      <td>1.406571</td>\n      <td>50.190000</td>\n      <td>26.160000</td>\n      <td>41.860000</td>\n      <td>41.790000</td>\n      <td>20.007335</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.388400</td>\n      <td>1.377837</td>\n      <td>51.530000</td>\n      <td>27.590000</td>\n      <td>43.080000</td>\n      <td>43.070000</td>\n      <td>20.169927</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.337400</td>\n      <td>1.366906</td>\n      <td>51.610000</td>\n      <td>27.710000</td>\n      <td>43.410000</td>\n      <td>43.370000</td>\n      <td>20.086797</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3682, training_loss=1.451196497513638, metrics={'train_runtime': 6236.8076, 'train_samples_per_second': 4.724, 'train_steps_per_second': 0.59, 'total_flos': 1.1014711712268288e+16, 'train_loss': 1.451196497513638, 'epoch': 1.9995926956757857})"},"metadata":{}}],"execution_count":13},{"id":"80e20767","cell_type":"markdown","source":"## 8) ÄÃ¡nh giÃ¡","metadata":{}},{"id":"e2f5d760","cell_type":"code","source":"val_metrics = trainer.evaluate(eval_dataset=processed_datasets[\"validation\"])\nprint(\"Validation metrics:\", val_metrics)\n\ntest_metrics = trainer.evaluate(eval_dataset=processed_datasets[\"test\"])\nprint(\"Test metrics:\", test_metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T10:46:54.551314Z","iopub.execute_input":"2025-10-04T10:46:54.551651Z","iopub.status.idle":"2025-10-04T11:02:00.234759Z","shell.execute_reply.started":"2025-10-04T10:46:54.551622Z","shell.execute_reply":"2025-10-04T11:02:00.233826Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='819' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [409/409 15:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation metrics: {'eval_loss': 1.3669055700302124, 'eval_rouge1': 51.58, 'eval_rouge2': 27.67, 'eval_rougeL': 43.43, 'eval_rougeLsum': 43.42, 'eval_gen_len': 20.086797066014668, 'eval_runtime': 447.1285, 'eval_samples_per_second': 1.829, 'eval_steps_per_second': 0.915, 'epoch': 1.9995926956757857}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Test metrics: {'eval_loss': 1.3822301626205444, 'eval_rouge1': 50.85, 'eval_rouge2': 26.89, 'eval_rougeL': 42.85, 'eval_rougeLsum': 42.89, 'eval_gen_len': 19.835164835164836, 'eval_runtime': 458.5388, 'eval_samples_per_second': 1.786, 'eval_steps_per_second': 0.894, 'epoch': 1.9995926956757857}\n","output_type":"stream"}],"execution_count":14},{"id":"f5addba2","cell_type":"markdown","source":"## 9) Sinh tÃ³m táº¯t máº«u","metadata":{}},{"id":"2c4d6652","cell_type":"code","source":"def generate_summary(dialogue, max_new_tokens=128, num_beams=4):\n    inputs = tokenizer(\n        dialogue,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=max_input_length\n    ).to(device)\n    with torch.no_grad():\n        summary_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            num_beams=num_beams,\n            length_penalty=1.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nsample_dialogue = dataset[\"test\"][0][\"dialogue\"]\nsample_ref      = dataset[\"test\"][0][\"summary\"]\n\nprint(\"DIALOGUE:\\n\", sample_dialogue[:800], \"...\\n\")\nprint(\"REFERENCE SUMMARY:\\n\", sample_ref, \"\\n\")\n\npred = generate_summary(sample_dialogue)\nprint(\"MODEL SUMMARY:\\n\", pred)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:10:49.942205Z","iopub.execute_input":"2025-10-04T11:10:49.942650Z","iopub.status.idle":"2025-10-04T11:10:50.543274Z","shell.execute_reply.started":"2025-10-04T11:10:49.942627Z","shell.execute_reply":"2025-10-04T11:10:50.542680Z"}},"outputs":[{"name":"stdout","text":"DIALOGUE:\n Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him ğŸ™‚\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye ...\n\nREFERENCE SUMMARY:\n Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry. \n\nMODEL SUMMARY:\n Amanda can't find Betty's number. Hannah wants Amanda to text Larry.\n","output_type":"stream"}],"execution_count":17},{"id":"80ff8b6d","cell_type":"markdown","source":"## 10) LÆ°u model & tokenizer","metadata":{}},{"id":"55b68891","cell_type":"code","source":"trainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(\"Saved to:\", output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:12:18.807698Z","iopub.execute_input":"2025-10-04T11:12:18.808481Z","iopub.status.idle":"2025-10-04T11:12:22.896446Z","shell.execute_reply.started":"2025-10-04T11:12:18.808455Z","shell.execute_reply":"2025-10-04T11:12:22.895698Z"}},"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n","output_type":"stream"},{"name":"stdout","text":"Saved to: pegasus-samsum-finetuned\n","output_type":"stream"}],"execution_count":18},{"id":"1d3d5c71-c1f1-42f6-91fe-b25691db8384","cell_type":"markdown","source":"## 11) Test","metadata":{}},{"id":"0447d1c1-9f16-43bb-88a4-45d0a17773da","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# ğŸ”¸ Chá»n checkpoint báº¡n muá»‘n test\nmodel_dir = \"/kaggle/working/pegasus-samsum-finetuned/checkpoint-3682\"\n\n# ğŸ”¹ Load model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# ğŸ”¹ HÃ m sinh tÃ³m táº¯t\ndef summarize_text(text, max_input_length=512, max_new_tokens=128, num_beams=4):\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        max_length=max_input_length,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    with torch.no_grad():\n        summary_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            num_beams=num_beams,\n            length_penalty=1.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# ğŸ”¹ VÃ­ dá»¥: Ä‘oáº¡n vÄƒn (khÃ´ng pháº£i há»™i thoáº¡i)\ntext = \"\"\"\nArtificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications ranging from language translation and image recognition to autonomous vehicles and healthcare diagnostics. \nHowever, the widespread adoption of AI also raises ethical concerns, including bias in algorithms, data privacy, and job displacement. \nGovernments and organizations are now working to establish regulations that ensure the safe and fair use of AI technologies.\n\"\"\"\n\nsummary = summarize_text(text)\nprint(\"ğŸ“„ Original Text:\\n\", text)\nprint(\"\\nğŸ“ Model Summary:\\n\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:46:10.171483Z","iopub.execute_input":"2025-10-04T11:46:10.172306Z","iopub.status.idle":"2025-10-04T11:46:15.846835Z","shell.execute_reply.started":"2025-10-04T11:46:10.172280Z","shell.execute_reply":"2025-10-04T11:46:15.846105Z"}},"outputs":[{"name":"stdout","text":"ğŸ“„ Original Text:\n \nArtificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications ranging from language translation and image recognition to autonomous vehicles and healthcare diagnostics. \nHowever, the widespread adoption of AI also raises ethical concerns, including bias in algorithms, data privacy, and job displacement. \nGovernments and organizations are now working to establish regulations that ensure the safe and fair use of AI technologies.\n\n\nğŸ“ Model Summary:\n Artificial intelligence (AI) has become a transformative force in the 21st century. Governments and organizations are working to establish regulations that ensure the safe and fair use of AI technologies.\n","output_type":"stream"}],"execution_count":24},{"id":"b7c35098-f7f1-4a95-b208-be7984677984","cell_type":"code","source":"# ===== 0) Setup & load model =====\nimport os, re, numpy as np, torch\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom rouge_score import rouge_scorer, scoring\n\n# >>> chá»n checkpoint Ä‘á»ƒ test:\nmodel_dir = \"/kaggle/working/pegasus-samsum-finetuned\"  # hoáº·c checkpoint-3682\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)\n\n# ===== 1) Utils =====\ndef summarize_text(text, max_input_length=512, max_new_tokens=96, num_beams=6,\n                   min_new_tokens=24, no_repeat_ngram_size=3, sampling=False):\n    enc = tokenizer(text, truncation=True, max_length=max_input_length, return_tensors=\"pt\").to(device)\n    gen_kwargs = dict(max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens,\n                      no_repeat_ngram_size=no_repeat_ngram_size)\n    if sampling:\n        gen_kwargs.update(dict(do_sample=True, temperature=0.9, top_p=0.9))\n    else:\n        gen_kwargs.update(dict(num_beams=num_beams, length_penalty=1.05, early_stopping=True))\n    with torch.no_grad():\n        out = model.generate(**enc, **gen_kwargs)\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\ndef tokens(s): \n    return re.findall(r\"\\w+\", s.lower(), flags=re.UNICODE)\n\ndef bigrams(ts): \n    return list(zip(ts, ts[1:]))\n\ndef quick_metrics(src, pred, ref=None):\n    src_toks, sum_toks = tokens(src), tokens(pred)\n    compression = len(sum_toks) / max(1, len(src_toks))\n    novelty = sum(1 for t in set(sum_toks) if t not in set(src_toks)) / max(1, len(set(sum_toks)))\n    bg = bigrams(sum_toks)\n    redundancy = 1 - (len(set(bg)) / max(1, len(bg)))\n    scores = {\"compression\": round(compression, 3),\n              \"novelty\": round(novelty, 3),\n              \"redundancy\": round(redundancy, 3)}\n    if ref is not None and len(ref.strip()) > 0:\n        scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'], use_stemmer=True)\n        agg = scoring.BootstrapAggregator()\n        agg.add_scores(scorer.score(ref, pred))\n        r = agg.aggregate()\n        scores.update({\n            \"rouge1\": round(r[\"rouge1\"].mid.fmeasure*100,2),\n            \"rouge2\": round(r[\"rouge2\"].mid.fmeasure*100,2),\n            \"rougeL\": round(r[\"rougeL\"].mid.fmeasure*100,2),\n            \"rougeLsum\": round(r[\"rougeLsum\"].mid.fmeasure*100,2),\n        })\n    return scores\n\ndef print_case(title, src, pred, met):\n    print(f\"\\n=== {title} ===\")\n    print(\"SOURCE:\", (src if len(src)<=400 else src[:400]+\" ...\"))\n    print(\"\\nSUMMARY:\", pred)\n    print(\"\\nMETRICS:\", met)\n\n# ===== 2) Test set =====\ntests = []\n\n# 2.1 Dialogue (in-domain) - casual\ntests.append(dict(\n    title=\"Dialogue - Casual (in-domain)\",\n    src=\"\"\"A: Hey, did you book the venue for Friday?\nB: Not yet, I'm comparing prices. \nA: We need it by tonight. \nB: Okay, I'll finalize the cheaper one and send you the receipt.\"\"\",\n    ref=\"They discuss booking a venue; B will choose the cheaper option and send the receipt.\"\n))\n\n# 2.2 Dialogue - multi-speaker with emoji/noise\ntests.append(dict(\n    title=\"Dialogue - Multi-speaker + emoji/noise\",\n    src=\"\"\"Ann: omg traffic today was insane ğŸ˜‚\nYi: told u to take the subway!!\nAnn: next time ğŸ˜­ btw 7pm dinner?\nBen: I'm in. where? \nYi: same place as last week. \"\"\",\n    ref=None\n))\n\n# 2.3 Expository paragraph (general)\ntests.append(dict(\n    title=\"Expository - General paragraph\",\n    src=\"\"\"Artificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications from translation and vision to autonomous vehicles and healthcare. \nYet widespread adoption raises concerns about bias, privacy, and job displacement. \nRegulators are crafting policies to ensure safe and fair AI.\"\"\",\n    ref=\"AI is reshaping multiple sectors but raises bias, privacy, and job concerns; regulators are creating safeguards.\"\n))\n\n# 2.4 Expository - Technical explainer (longer)\ntests.append(dict(\n    title=\"Expository - Technical explainer (long)\",\n    src=(\"\"\"Transformers rely on self-attention to weight tokens in a sequence. \nPositional encodings inject order information into token representations. \nDuring generation, beam search explores multiple candidates to balance quality and diversity. \nDespite strong results, long-context handling and hallucination remain open challenges.\"\"\")*3,  # replicate to exceed length\n    ref=None\n))\n\n# 2.5 News-like report\ntests.append(dict(\n    title=\"News - Short report\",\n    src=\"\"\"The city council approved a $5 million grant to expand bike lanes across downtown. \nOfficials expect construction to begin in March and finish by late summer, aiming to reduce congestion and emissions.\"\"\",\n    ref=\"City approves $5M to expand downtown bike lanes, with construction from March to late summer to cut congestion and emissions.\"\n))\n\n# 2.6 Email / Memo\ntests.append(dict(\n    title=\"Email - Work memo\",\n    src=\"\"\"Hi team,\nPlease review the Q3 draft by Thursday. Focus on the revenue projections and the assumptions for marketing spend.\nWe'll meet Friday at 10 AM to finalize. Thanks!\"\"\",\n    ref=\"Team must review Q3 draft by Thursday, focusing on revenue and marketing assumptions; final meeting Friday 10 AM.\"\n))\n\n# 2.7 Meeting Notes / Bullet list\ntests.append(dict(\n    title=\"Meeting Notes - Bullets\",\n    src=\"\"\"Agenda: \n- KPI review (conversion + retention)\n- Mobile release v1.2 next Tuesday\n- Assign owners for onboarding revamp\nAction items:\n- Alice: finalize copy\n- Bob: QA mobile build\n- Chen: dashboard metrics\"\"\",\n    ref=None\n))\n\n# 2.8 Noisy social text / code-mixed\ntests.append(dict(\n    title=\"Noisy - Code-mixed + teencode\",\n    src=\"\"\"Hnay deadline dÃ­ quÃ¡ nÃªn t ko ká»‹p push ğŸ˜…. I'll ping u táº§m 9pm, merge nhanh nhÃ©. Mai demo vs sáº¿p.\"\"\",\n    ref=\"Author will ping at 9pm to merge quickly; demo is tomorrow; deadline pressure today.\"\n))\n\n# 2.9 Very long input (forces truncation)\ntests.append(dict(\n    title=\"Very Long - Truncation check\",\n    src=(\"\"\"Chapter 1. Introduction... \"\"\" * 600),  # artificial long text\n    ref=None\n))\n\n# 2.10 Vietnamese expository\ntests.append(dict(\n    title=\"Vietnamese - Expository\",\n    src=\"\"\"CÃ´ng nghá»‡ chuá»—i khá»‘i cho phÃ©p lÆ°u trá»¯ dá»¯ liá»‡u theo dáº¡ng khá»‘i ná»‘i tiáº¿p, khÃ³ bá»‹ sá»­a Ä‘á»•i. \nNgoÃ i tiá»n mÃ£ hoÃ¡, blockchain cÃ²n Ä‘Æ°á»£c á»©ng dá»¥ng trong logistics, nháº­n dáº¡ng, vÃ  bá» phiáº¿u Ä‘iá»‡n tá»­. \nTuy nhiÃªn, cÃ¡c váº¥n Ä‘á» vá» má»Ÿ rá»™ng, phÃ­ giao dá»‹ch vÃ  tiÃªu thá»¥ nÄƒng lÆ°á»£ng váº«n lÃ  rÃ o cáº£n lá»›n.\"\"\",\n    ref=\"Blockchain lÆ°u trá»¯ dá»¯ liá»‡u khÃ³ chá»‰nh sá»­a vÃ  cÃ³ á»©ng dá»¥ng rá»™ng, nhÆ°ng vÆ°á»›ng má»Ÿ rá»™ng, phÃ­ vÃ  nÄƒng lÆ°á»£ng.\"\n))\n\n# 2.11 Vietnamese dialogue\ntests.append(dict(\n    title=\"Vietnamese - Dialogue (in-domain-ish)\",\n    src=\"\"\"A: Tá»‘i nay há»p nhÃ³m máº¥y giá»?\nB: 8 giá» nha. \nA: Ok, t nhá»› share slide trÆ°á»›c nhÃ©.\nB: Rá»“i, t gá»­i trÆ°á»›c 7 rÆ°á»¡i.\"\"\",\n    ref=\"Há»p nhÃ³m lÃºc 8h; B sáº½ gá»­i slide trÆ°á»›c 7:30.\"\n))\n\n# ===== 3) Run all tests =====\nresults = []\nfor t in tests:\n    # Vá»›i vÄƒn báº£n ngoÃ i miá»n, thá»­ tÄƒng tÃ­nh trá»«u tÆ°á»£ng nháº¹\n    sampling = (\"Dialogue\" not in t[\"title\"])\n    pred = summarize_text(t[\"src\"], sampling=sampling)\n    met = quick_metrics(t[\"src\"], pred, t.get(\"ref\"))\n    results.append((t[\"title\"], pred, met))\n\n    print_case(t[\"title\"], t[\"src\"], pred, met)\n\n# Náº¿u muá»‘n xem báº£ng gá»n:\nimport pandas as pd\ndf = pd.DataFrame([\n    dict(title=title, **met) for (title, pred, met) in results\n])\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:52:02.067370Z","iopub.execute_input":"2025-10-04T11:52:02.067763Z","iopub.status.idle":"2025-10-04T11:52:26.843135Z","shell.execute_reply.started":"2025-10-04T11:52:02.067742Z","shell.execute_reply":"2025-10-04T11:52:26.842545Z"}},"outputs":[{"name":"stdout","text":"\n=== Dialogue - Casual (in-domain) ===\nSOURCE: A: Hey, did you book the venue for Friday?\nB: Not yet, I'm comparing prices. \nA: We need it by tonight. \nB: Okay, I'll finalize the cheaper one and send you the receipt.\n\nSUMMARY: B is comparing prices for the venue for Friday. He will finalize the cheaper one and send the receipt to A by tonight.\n\nMETRICS: {'compression': 0.657, 'novelty': 0.2, 'redundancy': 0.0, 'rouge1': 52.63, 'rouge2': 22.22, 'rougeL': 42.11, 'rougeLsum': 42.11}\n\n=== Dialogue - Multi-speaker + emoji/noise ===\nSOURCE: Ann: omg traffic today was insane ğŸ˜‚\nYi: told u to take the subway!!\nAnn: next time ğŸ˜­ btw 7pm dinner?\nBen: I'm in. where? \nYi: same place as last week. \n\nSUMMARY: Yi told Ann to take the subway. Ben is going to the same place as last week for dinner at 7 pm.\n\nMETRICS: {'compression': 0.733, 'novelty': 0.3, 'redundancy': 0.0}\n\n=== Expository - General paragraph ===\nSOURCE: Artificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications from translation and vision to autonomous vehicles and healthcare. \nYet widespread adoption raises concerns about bias, privacy, and job displacement. \nRegulators are crafting policies to ensure safe and fair AI.\n\nSUMMARY: Artificial intelligence (AI) has become a transformative force in the 21st century. Regulators are crafting policies to ensure safe and fair AI.\n\nMETRICS: {'compression': 0.489, 'novelty': 0.0, 'redundancy': 0.0, 'rouge1': 21.05, 'rouge2': 5.56, 'rougeL': 15.79, 'rougeLsum': 15.79}\n\n=== Expository - Technical explainer (long) ===\nSOURCE: Transformers rely on self-attention to weight tokens in a sequence. \nPositional encodings inject order information into token representations. \nDuring generation, beam search explores multiple candidates to balance quality and diversity. \nDespite strong results, long-context handling and hallucination remain open challenges.Transformers rely on self-attention to weight tokens in a sequence. \nPosit ...\n\nSUMMARY: Despite strong results, long-context handling and hallucination remain open challenges.Transformers rely on self-attention to weight tokens in a sequence.\n\nMETRICS: {'compression': 0.175, 'novelty': 0.0, 'redundancy': 0.0}\n\n=== News - Short report ===\nSOURCE: The city council approved a $5 million grant to expand bike lanes across downtown. \nOfficials expect construction to begin in March and finish by late summer, aiming to reduce congestion and emissions.\n\nSUMMARY: The city council approved a $5 million grant to expand bike lanes across downtown. Officials expect construction to begin in March and finish by late summer, aiming to reduce congestion and emissions.\n\nMETRICS: {'compression': 1.0, 'novelty': 0.0, 'redundancy': 0.0, 'rouge1': 61.54, 'rouge2': 20.0, 'rougeL': 53.85, 'rougeLsum': 53.85}\n\n=== Email - Work memo ===\nSOURCE: Hi team,\nPlease review the Q3 draft by Thursday. Focus on the revenue projections and the assumptions for marketing spend.\nWe'll meet Friday at 10 AM to finalize. Thanks!\n\nSUMMARY: We will meet Friday at 10 AM to review the Q3 draft. Focus on the revenue projections and the assumptions for marketing spend.\n\nMETRICS: {'compression': 0.767, 'novelty': 0.048, 'redundancy': 0.0, 'rouge1': 58.54, 'rouge2': 15.38, 'rougeL': 34.15, 'rougeLsum': 34.15}\n\n=== Meeting Notes - Bullets ===\nSOURCE: Agenda: \n- KPI review (conversion + retention)\n- Mobile release v1.2 next Tuesday\n- Assign owners for onboarding revamp\nAction items:\n- Alice: finalize copy\n- Bob: QA mobile build\n- Chen: dashboard metrics\n\nSUMMARY: Bob, Alice, Bob's QA mobile build and Chen's dashboard metrics will be discussed at the next meeting.\n\nMETRICS: {'compression': 0.679, 'novelty': 0.471, 'redundancy': 0.0}\n\n=== Noisy - Code-mixed + teencode ===\nSOURCE: Hnay deadline dÃ­ quÃ¡ nÃªn t ko ká»‹p push ğŸ˜…. I'll ping u táº§m 9pm, merge nhanh nhÃ©. Mai demo vs sáº¿p.\n\nSUMMARY: Hnay deadline d qu nn t ko kp push. I'll ping u at 9pm, merge nhanh nhÃ©. Mai demo vs sp.\n\nMETRICS: {'compression': 1.0, 'novelty': 0.273, 'redundancy': 0.0, 'rouge1': 33.33, 'rouge2': 5.88, 'rougeL': 27.78, 'rougeLsum': 27.78}\n\n=== Very Long - Truncation check ===\nSOURCE: Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introductio ...\n\nSUMMARY: Chapter 1. Chapter 2. Chapter 3. Chapter 4. Chapter 5. Chapter 6. Chapter 7. Chapter 8. Chapter 9. Chapter 10. Chapter 11. Chapter 12. Chapter 13. Chapter 14.\n\nMETRICS: {'compression': 0.016, 'novelty': 0.867, 'redundancy': 0.0}\n\n=== Vietnamese - Expository ===\nSOURCE: CÃ´ng nghá»‡ chuá»—i khá»‘i cho phÃ©p lÆ°u trá»¯ dá»¯ liá»‡u theo dáº¡ng khá»‘i ná»‘i tiáº¿p, khÃ³ bá»‹ sá»­a Ä‘á»•i. \nNgoÃ i tiá»n mÃ£ hoÃ¡, blockchain cÃ²n Ä‘Æ°á»£c á»©ng dá»¥ng trong logistics, nháº­n dáº¡ng, vÃ  bá» phiáº¿u Ä‘iá»‡n tá»­. \nTuy nhiÃªn, cÃ¡c váº¥n Ä‘á» vá» má»Ÿ rá»™ng, phÃ­ giao dá»‹ch vÃ  tiÃªu thá»¥ nÄƒng lÆ°á»£ng váº«n lÃ  rÃ o cáº£n lá»›n.\n\nSUMMARY: Cng ngh cho phÃ©p lu tr d liu theo dng khi ni tip.\n\nMETRICS: {'compression': 0.224, 'novelty': 0.769, 'redundancy': 0.0, 'rouge1': 13.04, 'rouge2': 4.55, 'rougeL': 8.7, 'rougeLsum': 8.7}\n\n=== Vietnamese - Dialogue (in-domain-ish) ===\nSOURCE: A: Tá»‘i nay há»p nhÃ³m máº¥y giá»?\nB: 8 giá» nha. \nA: Ok, t nhá»› share slide trÆ°á»›c nhÃ©.\nB: Rá»“i, t gá»­i trÆ°á»›c 7 rÆ°á»¡i.\n\nSUMMARY: Ti nay hp nhm my gi? B is 8 gi nha.\n\nMETRICS: {'compression': 0.423, 'novelty': 0.6, 'redundancy': 0.0, 'rouge1': 7.41, 'rouge2': 0.0, 'rougeL': 7.41, 'rougeLsum': 7.41}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                      title  compression  novelty  redundancy  \\\n0             Dialogue - Casual (in-domain)        0.657    0.200         0.0   \n1    Dialogue - Multi-speaker + emoji/noise        0.733    0.300         0.0   \n2            Expository - General paragraph        0.489    0.000         0.0   \n3   Expository - Technical explainer (long)        0.175    0.000         0.0   \n4                       News - Short report        1.000    0.000         0.0   \n5                         Email - Work memo        0.767    0.048         0.0   \n6                   Meeting Notes - Bullets        0.679    0.471         0.0   \n7             Noisy - Code-mixed + teencode        1.000    0.273         0.0   \n8              Very Long - Truncation check        0.016    0.867         0.0   \n9                   Vietnamese - Expository        0.224    0.769         0.0   \n10    Vietnamese - Dialogue (in-domain-ish)        0.423    0.600         0.0   \n\n    rouge1  rouge2  rougeL  rougeLsum  \n0    52.63   22.22   42.11      42.11  \n1      NaN     NaN     NaN        NaN  \n2    21.05    5.56   15.79      15.79  \n3      NaN     NaN     NaN        NaN  \n4    61.54   20.00   53.85      53.85  \n5    58.54   15.38   34.15      34.15  \n6      NaN     NaN     NaN        NaN  \n7    33.33    5.88   27.78      27.78  \n8      NaN     NaN     NaN        NaN  \n9    13.04    4.55    8.70       8.70  \n10    7.41    0.00    7.41       7.41  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>compression</th>\n      <th>novelty</th>\n      <th>redundancy</th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dialogue - Casual (in-domain)</td>\n      <td>0.657</td>\n      <td>0.200</td>\n      <td>0.0</td>\n      <td>52.63</td>\n      <td>22.22</td>\n      <td>42.11</td>\n      <td>42.11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dialogue - Multi-speaker + emoji/noise</td>\n      <td>0.733</td>\n      <td>0.300</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Expository - General paragraph</td>\n      <td>0.489</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>21.05</td>\n      <td>5.56</td>\n      <td>15.79</td>\n      <td>15.79</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Expository - Technical explainer (long)</td>\n      <td>0.175</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>News - Short report</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>61.54</td>\n      <td>20.00</td>\n      <td>53.85</td>\n      <td>53.85</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Email - Work memo</td>\n      <td>0.767</td>\n      <td>0.048</td>\n      <td>0.0</td>\n      <td>58.54</td>\n      <td>15.38</td>\n      <td>34.15</td>\n      <td>34.15</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Meeting Notes - Bullets</td>\n      <td>0.679</td>\n      <td>0.471</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Noisy - Code-mixed + teencode</td>\n      <td>1.000</td>\n      <td>0.273</td>\n      <td>0.0</td>\n      <td>33.33</td>\n      <td>5.88</td>\n      <td>27.78</td>\n      <td>27.78</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Very Long - Truncation check</td>\n      <td>0.016</td>\n      <td>0.867</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Vietnamese - Expository</td>\n      <td>0.224</td>\n      <td>0.769</td>\n      <td>0.0</td>\n      <td>13.04</td>\n      <td>4.55</td>\n      <td>8.70</td>\n      <td>8.70</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Vietnamese - Dialogue (in-domain-ish)</td>\n      <td>0.423</td>\n      <td>0.600</td>\n      <td>0.0</td>\n      <td>7.41</td>\n      <td>0.00</td>\n      <td>7.41</td>\n      <td>7.41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25}]}