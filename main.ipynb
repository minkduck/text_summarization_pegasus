{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ed60ae5c","cell_type":"markdown","source":"## 0) Cài đặt & Tương thích Kaggle","metadata":{}},{"id":"5b6adf16","cell_type":"code","source":"!pip -q install \"pyarrow<17,>=10\" \"datasets==2.20.0\" \"transformers==4.44.2\"                 \"evaluate==0.4.2\" \"accelerate>=0.31.0\" \"sentencepiece>=0.1.99\"                 \"rouge-score==0.1.2\" \"nltk>=3.8.1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:55:55.055862Z","iopub.execute_input":"2025-10-04T08:55:55.056482Z","iopub.status.idle":"2025-10-04T08:57:31.530218Z","shell.execute_reply.started":"2025-10-04T08:55:55.056445Z","shell.execute_reply":"2025-10-04T08:57:31.529546Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.5.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"744403de","cell_type":"markdown","source":"## 1) Import & Thiết lập","metadata":{}},{"id":"dd2d741e","cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport nltk\nnltk.download(\"punkt\")\n\nimport evaluate\nfrom datasets import load_dataset\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\n\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:57:31.531604Z","iopub.execute_input":"2025-10-04T08:57:31.531858Z","iopub.status.idle":"2025-10-04T08:57:54.887380Z","shell.execute_reply.started":"2025-10-04T08:57:31.531835Z","shell.execute_reply":"2025-10-04T08:57:54.886801Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n2025-10-04 08:57:42.359029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759568262.548193      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759568262.608666      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Torch: 2.6.0+cu124\nCUDA available: True\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":2},{"id":"dc605d67","cell_type":"markdown","source":"## 2) Tải dữ liệu SAMSum","metadata":{}},{"id":"41007078","cell_type":"code","source":"dataset = load_dataset(\"knkarthick/samsum\")\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:57:54.888080Z","iopub.execute_input":"2025-10-04T08:57:54.888703Z","iopub.status.idle":"2025-10-04T08:58:03.231233Z","shell.execute_reply.started":"2025-10-04T08:57:54.888683Z","shell.execute_reply":"2025-10-04T08:58:03.230545Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e40d609b9f4f1d88653f98c4b78945"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/9.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1afa38501458437b891144e9f51ed3b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/504k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"421050344ea149eab7395cd6587f08a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/522k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e8c2e1cd834de293f25c1e28b56ccd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14731 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef54fd36c4784383a9a848d47243fcbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc23239eaf6c435ba933d7a0fd0e9766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5573a9a3a049d489f11f87c54fa8ac"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14731\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n})"},"metadata":{}}],"execution_count":3},{"id":"974fbfff","cell_type":"markdown","source":"## 3) Cấu hình Tokenizer/Model","metadata":{}},{"id":"fd76baa4","cell_type":"code","source":"model_name = \"google/pegasus-large\"  # hoặc \"google/pegasus-samsum\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.to(device)\n\nmax_input_length  = 512\nmax_target_length = 128\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:58:03.232956Z","iopub.execute_input":"2025-10-04T08:58:03.233169Z","iopub.status.idle":"2025-10-04T08:58:25.954448Z","shell.execute_reply.started":"2025-10-04T08:58:03.233152Z","shell.execute_reply":"2025-10-04T08:58:25.953906Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47acdef6fde4d02bce322dbcc89f10a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bebd912ffa24d3bb6f528c32af447f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a20132147434d39859d2c23429be1f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0a0cf34ae194e6e9b78f2d2c265334b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49255282e4ac43c5853ac8c1c94646b5"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb8044c9a65b4069bd702c687ca4e552"}},"metadata":{}}],"execution_count":4},{"id":"6696f1bb","cell_type":"markdown","source":"## 4) Tiền xử lý","metadata":{}},{"id":"7557dee2","cell_type":"code","source":"def preprocess_function(batch):\n    inputs = batch[\"dialogue\"]\n    targets = batch[\"summary\"]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=max_target_length,\n            truncation=True\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprocessed_datasets = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\nprocessed_datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:58:25.955188Z","iopub.execute_input":"2025-10-04T08:58:25.955442Z","iopub.status.idle":"2025-10-04T08:58:31.517720Z","shell.execute_reply.started":"2025-10-04T08:58:25.955416Z","shell.execute_reply":"2025-10-04T08:58:31.517112Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14731 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6012c9950b6e45348e523664c5efc81e"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5064fb539ce046d294e2941708f61b4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2ed156684045c9ac2c5b4bf83db191"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 14731\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 818\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 819\n    })\n})"},"metadata":{}}],"execution_count":5},{"id":"a2e9c5a5","cell_type":"markdown","source":"## 5) Data Collator & ROUGE metric","metadata":{}},{"id":"fe93a61f","cell_type":"code","source":"from rouge_score import rouge_scorer, scoring\nimport numpy as np\n\n# Tạo scorer local (không cần mạng, không bị conflict tên thư mục)\n_scorer = rouge_scorer.RougeScorer(\n    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n    use_stemmer=True\n)\n\ndef postprocess_text(preds, labels):\n    preds = [p.strip() for p in preds]\n    labels = [l.strip() for l in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    # Decode\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # Tính ROUGE bằng bootstrap aggregator\n    aggregator = scoring.BootstrapAggregator()\n    for pred, ref in zip(decoded_preds, decoded_labels):\n        aggregator.add_scores(_scorer.score(ref, pred))\n    result = aggregator.aggregate()\n\n    # Lấy F1 (mid.fmeasure) và chuyển về %\n    out = {\n        \"rouge1\": round(result[\"rouge1\"].mid.fmeasure * 100, 2),\n        \"rouge2\": round(result[\"rouge2\"].mid.fmeasure * 100, 2),\n        \"rougeL\": round(result[\"rougeL\"].mid.fmeasure * 100, 2),\n        \"rougeLsum\": round(result[\"rougeLsum\"].mid.fmeasure * 100, 2),\n    }\n\n    # Độ dài trung bình của chuỗi sinh ra\n    prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n    out[\"gen_len\"] = float(np.mean(prediction_lens))\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T08:58:31.518528Z","iopub.execute_input":"2025-10-04T08:58:31.518795Z","iopub.status.idle":"2025-10-04T08:58:31.530306Z","shell.execute_reply.started":"2025-10-04T08:58:31.518777Z","shell.execute_reply":"2025-10-04T08:58:31.529621Z"}},"outputs":[],"execution_count":6},{"id":"b231b125","cell_type":"markdown","source":"## 6) Thiết lập tham số huấn luyện","metadata":{}},{"id":"b1e83def","cell_type":"code","source":"output_dir = \"pegasus-samsum-finetuned\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    evaluation_strategy=\"steps\",\n    eval_steps=1000,\n    save_steps=1000,\n    logging_steps=200,\n    save_total_limit=2,\n    learning_rate=5e-5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=2,\n    predict_with_generate=True,\n    generation_max_length=max_target_length,\n    fp16=torch.cuda.is_available(),\n    dataloader_num_workers=2,\n    report_to=\"none\",\n    metric_for_best_model=\"rougeLsum\",\n    load_best_model_at_end=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:01:20.471886Z","iopub.execute_input":"2025-10-04T09:01:20.472183Z","iopub.status.idle":"2025-10-04T09:01:20.499798Z","shell.execute_reply.started":"2025-10-04T09:01:20.472161Z","shell.execute_reply":"2025-10-04T09:01:20.498951Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"id":"de7a6000","cell_type":"markdown","source":"## 7) Huấn luyện","metadata":{}},{"id":"3bcec6e7","cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=processed_datasets[\"train\"],\n    eval_dataset=processed_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n    compute_metrics=compute_metrics,\n)\n\ntrain_result = trainer.train()\ntrain_result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:02:44.035367Z","iopub.execute_input":"2025-10-04T09:02:44.036278Z","iopub.status.idle":"2025-10-04T10:46:41.334829Z","shell.execute_reply.started":"2025-10-04T09:02:44.036243Z","shell.execute_reply":"2025-10-04T10:46:41.334010Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3682' max='3682' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3682/3682 1:43:54, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>1.488700</td>\n      <td>1.406571</td>\n      <td>50.190000</td>\n      <td>26.160000</td>\n      <td>41.860000</td>\n      <td>41.790000</td>\n      <td>20.007335</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.388400</td>\n      <td>1.377837</td>\n      <td>51.530000</td>\n      <td>27.590000</td>\n      <td>43.080000</td>\n      <td>43.070000</td>\n      <td>20.169927</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.337400</td>\n      <td>1.366906</td>\n      <td>51.610000</td>\n      <td>27.710000</td>\n      <td>43.410000</td>\n      <td>43.370000</td>\n      <td>20.086797</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3682, training_loss=1.451196497513638, metrics={'train_runtime': 6236.8076, 'train_samples_per_second': 4.724, 'train_steps_per_second': 0.59, 'total_flos': 1.1014711712268288e+16, 'train_loss': 1.451196497513638, 'epoch': 1.9995926956757857})"},"metadata":{}}],"execution_count":13},{"id":"80e20767","cell_type":"markdown","source":"## 8) Đánh giá","metadata":{}},{"id":"e2f5d760","cell_type":"code","source":"val_metrics = trainer.evaluate(eval_dataset=processed_datasets[\"validation\"])\nprint(\"Validation metrics:\", val_metrics)\n\ntest_metrics = trainer.evaluate(eval_dataset=processed_datasets[\"test\"])\nprint(\"Test metrics:\", test_metrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T10:46:54.551314Z","iopub.execute_input":"2025-10-04T10:46:54.551651Z","iopub.status.idle":"2025-10-04T11:02:00.234759Z","shell.execute_reply.started":"2025-10-04T10:46:54.551622Z","shell.execute_reply":"2025-10-04T11:02:00.233826Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='819' max='409' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [409/409 15:02]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation metrics: {'eval_loss': 1.3669055700302124, 'eval_rouge1': 51.58, 'eval_rouge2': 27.67, 'eval_rougeL': 43.43, 'eval_rougeLsum': 43.42, 'eval_gen_len': 20.086797066014668, 'eval_runtime': 447.1285, 'eval_samples_per_second': 1.829, 'eval_steps_per_second': 0.915, 'epoch': 1.9995926956757857}\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Test metrics: {'eval_loss': 1.3822301626205444, 'eval_rouge1': 50.85, 'eval_rouge2': 26.89, 'eval_rougeL': 42.85, 'eval_rougeLsum': 42.89, 'eval_gen_len': 19.835164835164836, 'eval_runtime': 458.5388, 'eval_samples_per_second': 1.786, 'eval_steps_per_second': 0.894, 'epoch': 1.9995926956757857}\n","output_type":"stream"}],"execution_count":14},{"id":"f5addba2","cell_type":"markdown","source":"## 9) Sinh tóm tắt mẫu","metadata":{}},{"id":"2c4d6652","cell_type":"code","source":"def generate_summary(dialogue, max_new_tokens=128, num_beams=4):\n    inputs = tokenizer(\n        dialogue,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=max_input_length\n    ).to(device)\n    with torch.no_grad():\n        summary_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            num_beams=num_beams,\n            length_penalty=1.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\nsample_dialogue = dataset[\"test\"][0][\"dialogue\"]\nsample_ref      = dataset[\"test\"][0][\"summary\"]\n\nprint(\"DIALOGUE:\\n\", sample_dialogue[:800], \"...\\n\")\nprint(\"REFERENCE SUMMARY:\\n\", sample_ref, \"\\n\")\n\npred = generate_summary(sample_dialogue)\nprint(\"MODEL SUMMARY:\\n\", pred)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:10:49.942205Z","iopub.execute_input":"2025-10-04T11:10:49.942650Z","iopub.status.idle":"2025-10-04T11:10:50.543274Z","shell.execute_reply.started":"2025-10-04T11:10:49.942627Z","shell.execute_reply":"2025-10-04T11:10:50.542680Z"}},"outputs":[{"name":"stdout","text":"DIALOGUE:\n Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye ...\n\nREFERENCE SUMMARY:\n Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry. \n\nMODEL SUMMARY:\n Amanda can't find Betty's number. Hannah wants Amanda to text Larry.\n","output_type":"stream"}],"execution_count":17},{"id":"80ff8b6d","cell_type":"markdown","source":"## 10) Lưu model & tokenizer","metadata":{}},{"id":"55b68891","cell_type":"code","source":"trainer.save_model(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(\"Saved to:\", output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:12:18.807698Z","iopub.execute_input":"2025-10-04T11:12:18.808481Z","iopub.status.idle":"2025-10-04T11:12:22.896446Z","shell.execute_reply.started":"2025-10-04T11:12:18.808455Z","shell.execute_reply":"2025-10-04T11:12:22.895698Z"}},"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 256, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n","output_type":"stream"},{"name":"stdout","text":"Saved to: pegasus-samsum-finetuned\n","output_type":"stream"}],"execution_count":18},{"id":"1d3d5c71-c1f1-42f6-91fe-b25691db8384","cell_type":"markdown","source":"## 11) Test","metadata":{}},{"id":"0447d1c1-9f16-43bb-88a4-45d0a17773da","cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\n# 🔸 Chọn checkpoint bạn muốn test\nmodel_dir = \"/kaggle/working/pegasus-samsum-finetuned/checkpoint-3682\"\n\n# 🔹 Load model + tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 🔹 Hàm sinh tóm tắt\ndef summarize_text(text, max_input_length=512, max_new_tokens=128, num_beams=4):\n    inputs = tokenizer(\n        text,\n        truncation=True,\n        max_length=max_input_length,\n        return_tensors=\"pt\"\n    ).to(model.device)\n    with torch.no_grad():\n        summary_ids = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            num_beams=num_beams,\n            length_penalty=1.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# 🔹 Ví dụ: đoạn văn (không phải hội thoại)\ntext = \"\"\"\nArtificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications ranging from language translation and image recognition to autonomous vehicles and healthcare diagnostics. \nHowever, the widespread adoption of AI also raises ethical concerns, including bias in algorithms, data privacy, and job displacement. \nGovernments and organizations are now working to establish regulations that ensure the safe and fair use of AI technologies.\n\"\"\"\n\nsummary = summarize_text(text)\nprint(\"📄 Original Text:\\n\", text)\nprint(\"\\n📝 Model Summary:\\n\", summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:46:10.171483Z","iopub.execute_input":"2025-10-04T11:46:10.172306Z","iopub.status.idle":"2025-10-04T11:46:15.846835Z","shell.execute_reply.started":"2025-10-04T11:46:10.172280Z","shell.execute_reply":"2025-10-04T11:46:15.846105Z"}},"outputs":[{"name":"stdout","text":"📄 Original Text:\n \nArtificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications ranging from language translation and image recognition to autonomous vehicles and healthcare diagnostics. \nHowever, the widespread adoption of AI also raises ethical concerns, including bias in algorithms, data privacy, and job displacement. \nGovernments and organizations are now working to establish regulations that ensure the safe and fair use of AI technologies.\n\n\n📝 Model Summary:\n Artificial intelligence (AI) has become a transformative force in the 21st century. Governments and organizations are working to establish regulations that ensure the safe and fair use of AI technologies.\n","output_type":"stream"}],"execution_count":24},{"id":"b7c35098-f7f1-4a95-b208-be7984677984","cell_type":"code","source":"# ===== 0) Setup & load model =====\nimport os, re, numpy as np, torch\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom rouge_score import rouge_scorer, scoring\n\n# >>> chọn checkpoint để test:\nmodel_dir = \"/kaggle/working/pegasus-samsum-finetuned\"  # hoặc checkpoint-3682\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device)\n\n# ===== 1) Utils =====\ndef summarize_text(text, max_input_length=512, max_new_tokens=96, num_beams=6,\n                   min_new_tokens=24, no_repeat_ngram_size=3, sampling=False):\n    enc = tokenizer(text, truncation=True, max_length=max_input_length, return_tensors=\"pt\").to(device)\n    gen_kwargs = dict(max_new_tokens=max_new_tokens, min_new_tokens=min_new_tokens,\n                      no_repeat_ngram_size=no_repeat_ngram_size)\n    if sampling:\n        gen_kwargs.update(dict(do_sample=True, temperature=0.9, top_p=0.9))\n    else:\n        gen_kwargs.update(dict(num_beams=num_beams, length_penalty=1.05, early_stopping=True))\n    with torch.no_grad():\n        out = model.generate(**enc, **gen_kwargs)\n    return tokenizer.decode(out[0], skip_special_tokens=True)\n\ndef tokens(s): \n    return re.findall(r\"\\w+\", s.lower(), flags=re.UNICODE)\n\ndef bigrams(ts): \n    return list(zip(ts, ts[1:]))\n\ndef quick_metrics(src, pred, ref=None):\n    src_toks, sum_toks = tokens(src), tokens(pred)\n    compression = len(sum_toks) / max(1, len(src_toks))\n    novelty = sum(1 for t in set(sum_toks) if t not in set(src_toks)) / max(1, len(set(sum_toks)))\n    bg = bigrams(sum_toks)\n    redundancy = 1 - (len(set(bg)) / max(1, len(bg)))\n    scores = {\"compression\": round(compression, 3),\n              \"novelty\": round(novelty, 3),\n              \"redundancy\": round(redundancy, 3)}\n    if ref is not None and len(ref.strip()) > 0:\n        scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL','rougeLsum'], use_stemmer=True)\n        agg = scoring.BootstrapAggregator()\n        agg.add_scores(scorer.score(ref, pred))\n        r = agg.aggregate()\n        scores.update({\n            \"rouge1\": round(r[\"rouge1\"].mid.fmeasure*100,2),\n            \"rouge2\": round(r[\"rouge2\"].mid.fmeasure*100,2),\n            \"rougeL\": round(r[\"rougeL\"].mid.fmeasure*100,2),\n            \"rougeLsum\": round(r[\"rougeLsum\"].mid.fmeasure*100,2),\n        })\n    return scores\n\ndef print_case(title, src, pred, met):\n    print(f\"\\n=== {title} ===\")\n    print(\"SOURCE:\", (src if len(src)<=400 else src[:400]+\" ...\"))\n    print(\"\\nSUMMARY:\", pred)\n    print(\"\\nMETRICS:\", met)\n\n# ===== 2) Test set =====\ntests = []\n\n# 2.1 Dialogue (in-domain) - casual\ntests.append(dict(\n    title=\"Dialogue - Casual (in-domain)\",\n    src=\"\"\"A: Hey, did you book the venue for Friday?\nB: Not yet, I'm comparing prices. \nA: We need it by tonight. \nB: Okay, I'll finalize the cheaper one and send you the receipt.\"\"\",\n    ref=\"They discuss booking a venue; B will choose the cheaper option and send the receipt.\"\n))\n\n# 2.2 Dialogue - multi-speaker with emoji/noise\ntests.append(dict(\n    title=\"Dialogue - Multi-speaker + emoji/noise\",\n    src=\"\"\"Ann: omg traffic today was insane 😂\nYi: told u to take the subway!!\nAnn: next time 😭 btw 7pm dinner?\nBen: I'm in. where? \nYi: same place as last week. \"\"\",\n    ref=None\n))\n\n# 2.3 Expository paragraph (general)\ntests.append(dict(\n    title=\"Expository - General paragraph\",\n    src=\"\"\"Artificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications from translation and vision to autonomous vehicles and healthcare. \nYet widespread adoption raises concerns about bias, privacy, and job displacement. \nRegulators are crafting policies to ensure safe and fair AI.\"\"\",\n    ref=\"AI is reshaping multiple sectors but raises bias, privacy, and job concerns; regulators are creating safeguards.\"\n))\n\n# 2.4 Expository - Technical explainer (longer)\ntests.append(dict(\n    title=\"Expository - Technical explainer (long)\",\n    src=(\"\"\"Transformers rely on self-attention to weight tokens in a sequence. \nPositional encodings inject order information into token representations. \nDuring generation, beam search explores multiple candidates to balance quality and diversity. \nDespite strong results, long-context handling and hallucination remain open challenges.\"\"\")*3,  # replicate to exceed length\n    ref=None\n))\n\n# 2.5 News-like report\ntests.append(dict(\n    title=\"News - Short report\",\n    src=\"\"\"The city council approved a $5 million grant to expand bike lanes across downtown. \nOfficials expect construction to begin in March and finish by late summer, aiming to reduce congestion and emissions.\"\"\",\n    ref=\"City approves $5M to expand downtown bike lanes, with construction from March to late summer to cut congestion and emissions.\"\n))\n\n# 2.6 Email / Memo\ntests.append(dict(\n    title=\"Email - Work memo\",\n    src=\"\"\"Hi team,\nPlease review the Q3 draft by Thursday. Focus on the revenue projections and the assumptions for marketing spend.\nWe'll meet Friday at 10 AM to finalize. Thanks!\"\"\",\n    ref=\"Team must review Q3 draft by Thursday, focusing on revenue and marketing assumptions; final meeting Friday 10 AM.\"\n))\n\n# 2.7 Meeting Notes / Bullet list\ntests.append(dict(\n    title=\"Meeting Notes - Bullets\",\n    src=\"\"\"Agenda: \n- KPI review (conversion + retention)\n- Mobile release v1.2 next Tuesday\n- Assign owners for onboarding revamp\nAction items:\n- Alice: finalize copy\n- Bob: QA mobile build\n- Chen: dashboard metrics\"\"\",\n    ref=None\n))\n\n# 2.8 Noisy social text / code-mixed\ntests.append(dict(\n    title=\"Noisy - Code-mixed + teencode\",\n    src=\"\"\"Hnay deadline dí quá nên t ko kịp push 😅. I'll ping u tầm 9pm, merge nhanh nhé. Mai demo vs sếp.\"\"\",\n    ref=\"Author will ping at 9pm to merge quickly; demo is tomorrow; deadline pressure today.\"\n))\n\n# 2.9 Very long input (forces truncation)\ntests.append(dict(\n    title=\"Very Long - Truncation check\",\n    src=(\"\"\"Chapter 1. Introduction... \"\"\" * 600),  # artificial long text\n    ref=None\n))\n\n# 2.10 Vietnamese expository\ntests.append(dict(\n    title=\"Vietnamese - Expository\",\n    src=\"\"\"Công nghệ chuỗi khối cho phép lưu trữ dữ liệu theo dạng khối nối tiếp, khó bị sửa đổi. \nNgoài tiền mã hoá, blockchain còn được ứng dụng trong logistics, nhận dạng, và bỏ phiếu điện tử. \nTuy nhiên, các vấn đề về mở rộng, phí giao dịch và tiêu thụ năng lượng vẫn là rào cản lớn.\"\"\",\n    ref=\"Blockchain lưu trữ dữ liệu khó chỉnh sửa và có ứng dụng rộng, nhưng vướng mở rộng, phí và năng lượng.\"\n))\n\n# 2.11 Vietnamese dialogue\ntests.append(dict(\n    title=\"Vietnamese - Dialogue (in-domain-ish)\",\n    src=\"\"\"A: Tối nay họp nhóm mấy giờ?\nB: 8 giờ nha. \nA: Ok, t nhớ share slide trước nhé.\nB: Rồi, t gửi trước 7 rưỡi.\"\"\",\n    ref=\"Họp nhóm lúc 8h; B sẽ gửi slide trước 7:30.\"\n))\n\n# ===== 3) Run all tests =====\nresults = []\nfor t in tests:\n    # Với văn bản ngoài miền, thử tăng tính trừu tượng nhẹ\n    sampling = (\"Dialogue\" not in t[\"title\"])\n    pred = summarize_text(t[\"src\"], sampling=sampling)\n    met = quick_metrics(t[\"src\"], pred, t.get(\"ref\"))\n    results.append((t[\"title\"], pred, met))\n\n    print_case(t[\"title\"], t[\"src\"], pred, met)\n\n# Nếu muốn xem bảng gọn:\nimport pandas as pd\ndf = pd.DataFrame([\n    dict(title=title, **met) for (title, pred, met) in results\n])\ndf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T11:52:02.067370Z","iopub.execute_input":"2025-10-04T11:52:02.067763Z","iopub.status.idle":"2025-10-04T11:52:26.843135Z","shell.execute_reply.started":"2025-10-04T11:52:02.067742Z","shell.execute_reply":"2025-10-04T11:52:26.842545Z"}},"outputs":[{"name":"stdout","text":"\n=== Dialogue - Casual (in-domain) ===\nSOURCE: A: Hey, did you book the venue for Friday?\nB: Not yet, I'm comparing prices. \nA: We need it by tonight. \nB: Okay, I'll finalize the cheaper one and send you the receipt.\n\nSUMMARY: B is comparing prices for the venue for Friday. He will finalize the cheaper one and send the receipt to A by tonight.\n\nMETRICS: {'compression': 0.657, 'novelty': 0.2, 'redundancy': 0.0, 'rouge1': 52.63, 'rouge2': 22.22, 'rougeL': 42.11, 'rougeLsum': 42.11}\n\n=== Dialogue - Multi-speaker + emoji/noise ===\nSOURCE: Ann: omg traffic today was insane 😂\nYi: told u to take the subway!!\nAnn: next time 😭 btw 7pm dinner?\nBen: I'm in. where? \nYi: same place as last week. \n\nSUMMARY: Yi told Ann to take the subway. Ben is going to the same place as last week for dinner at 7 pm.\n\nMETRICS: {'compression': 0.733, 'novelty': 0.3, 'redundancy': 0.0}\n\n=== Expository - General paragraph ===\nSOURCE: Artificial intelligence (AI) has become a transformative force in the 21st century. \nIt powers applications from translation and vision to autonomous vehicles and healthcare. \nYet widespread adoption raises concerns about bias, privacy, and job displacement. \nRegulators are crafting policies to ensure safe and fair AI.\n\nSUMMARY: Artificial intelligence (AI) has become a transformative force in the 21st century. Regulators are crafting policies to ensure safe and fair AI.\n\nMETRICS: {'compression': 0.489, 'novelty': 0.0, 'redundancy': 0.0, 'rouge1': 21.05, 'rouge2': 5.56, 'rougeL': 15.79, 'rougeLsum': 15.79}\n\n=== Expository - Technical explainer (long) ===\nSOURCE: Transformers rely on self-attention to weight tokens in a sequence. \nPositional encodings inject order information into token representations. \nDuring generation, beam search explores multiple candidates to balance quality and diversity. \nDespite strong results, long-context handling and hallucination remain open challenges.Transformers rely on self-attention to weight tokens in a sequence. \nPosit ...\n\nSUMMARY: Despite strong results, long-context handling and hallucination remain open challenges.Transformers rely on self-attention to weight tokens in a sequence.\n\nMETRICS: {'compression': 0.175, 'novelty': 0.0, 'redundancy': 0.0}\n\n=== News - Short report ===\nSOURCE: The city council approved a $5 million grant to expand bike lanes across downtown. \nOfficials expect construction to begin in March and finish by late summer, aiming to reduce congestion and emissions.\n\nSUMMARY: The city council approved a $5 million grant to expand bike lanes across downtown. Officials expect construction to begin in March and finish by late summer, aiming to reduce congestion and emissions.\n\nMETRICS: {'compression': 1.0, 'novelty': 0.0, 'redundancy': 0.0, 'rouge1': 61.54, 'rouge2': 20.0, 'rougeL': 53.85, 'rougeLsum': 53.85}\n\n=== Email - Work memo ===\nSOURCE: Hi team,\nPlease review the Q3 draft by Thursday. Focus on the revenue projections and the assumptions for marketing spend.\nWe'll meet Friday at 10 AM to finalize. Thanks!\n\nSUMMARY: We will meet Friday at 10 AM to review the Q3 draft. Focus on the revenue projections and the assumptions for marketing spend.\n\nMETRICS: {'compression': 0.767, 'novelty': 0.048, 'redundancy': 0.0, 'rouge1': 58.54, 'rouge2': 15.38, 'rougeL': 34.15, 'rougeLsum': 34.15}\n\n=== Meeting Notes - Bullets ===\nSOURCE: Agenda: \n- KPI review (conversion + retention)\n- Mobile release v1.2 next Tuesday\n- Assign owners for onboarding revamp\nAction items:\n- Alice: finalize copy\n- Bob: QA mobile build\n- Chen: dashboard metrics\n\nSUMMARY: Bob, Alice, Bob's QA mobile build and Chen's dashboard metrics will be discussed at the next meeting.\n\nMETRICS: {'compression': 0.679, 'novelty': 0.471, 'redundancy': 0.0}\n\n=== Noisy - Code-mixed + teencode ===\nSOURCE: Hnay deadline dí quá nên t ko kịp push 😅. I'll ping u tầm 9pm, merge nhanh nhé. Mai demo vs sếp.\n\nSUMMARY: Hnay deadline d qu nn t ko kp push. I'll ping u at 9pm, merge nhanh nhé. Mai demo vs sp.\n\nMETRICS: {'compression': 1.0, 'novelty': 0.273, 'redundancy': 0.0, 'rouge1': 33.33, 'rouge2': 5.88, 'rougeL': 27.78, 'rougeLsum': 27.78}\n\n=== Very Long - Truncation check ===\nSOURCE: Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introduction... Chapter 1. Introductio ...\n\nSUMMARY: Chapter 1. Chapter 2. Chapter 3. Chapter 4. Chapter 5. Chapter 6. Chapter 7. Chapter 8. Chapter 9. Chapter 10. Chapter 11. Chapter 12. Chapter 13. Chapter 14.\n\nMETRICS: {'compression': 0.016, 'novelty': 0.867, 'redundancy': 0.0}\n\n=== Vietnamese - Expository ===\nSOURCE: Công nghệ chuỗi khối cho phép lưu trữ dữ liệu theo dạng khối nối tiếp, khó bị sửa đổi. \nNgoài tiền mã hoá, blockchain còn được ứng dụng trong logistics, nhận dạng, và bỏ phiếu điện tử. \nTuy nhiên, các vấn đề về mở rộng, phí giao dịch và tiêu thụ năng lượng vẫn là rào cản lớn.\n\nSUMMARY: Cng ngh cho phép lu tr d liu theo dng khi ni tip.\n\nMETRICS: {'compression': 0.224, 'novelty': 0.769, 'redundancy': 0.0, 'rouge1': 13.04, 'rouge2': 4.55, 'rougeL': 8.7, 'rougeLsum': 8.7}\n\n=== Vietnamese - Dialogue (in-domain-ish) ===\nSOURCE: A: Tối nay họp nhóm mấy giờ?\nB: 8 giờ nha. \nA: Ok, t nhớ share slide trước nhé.\nB: Rồi, t gửi trước 7 rưỡi.\n\nSUMMARY: Ti nay hp nhm my gi? B is 8 gi nha.\n\nMETRICS: {'compression': 0.423, 'novelty': 0.6, 'redundancy': 0.0, 'rouge1': 7.41, 'rouge2': 0.0, 'rougeL': 7.41, 'rougeLsum': 7.41}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                      title  compression  novelty  redundancy  \\\n0             Dialogue - Casual (in-domain)        0.657    0.200         0.0   \n1    Dialogue - Multi-speaker + emoji/noise        0.733    0.300         0.0   \n2            Expository - General paragraph        0.489    0.000         0.0   \n3   Expository - Technical explainer (long)        0.175    0.000         0.0   \n4                       News - Short report        1.000    0.000         0.0   \n5                         Email - Work memo        0.767    0.048         0.0   \n6                   Meeting Notes - Bullets        0.679    0.471         0.0   \n7             Noisy - Code-mixed + teencode        1.000    0.273         0.0   \n8              Very Long - Truncation check        0.016    0.867         0.0   \n9                   Vietnamese - Expository        0.224    0.769         0.0   \n10    Vietnamese - Dialogue (in-domain-ish)        0.423    0.600         0.0   \n\n    rouge1  rouge2  rougeL  rougeLsum  \n0    52.63   22.22   42.11      42.11  \n1      NaN     NaN     NaN        NaN  \n2    21.05    5.56   15.79      15.79  \n3      NaN     NaN     NaN        NaN  \n4    61.54   20.00   53.85      53.85  \n5    58.54   15.38   34.15      34.15  \n6      NaN     NaN     NaN        NaN  \n7    33.33    5.88   27.78      27.78  \n8      NaN     NaN     NaN        NaN  \n9    13.04    4.55    8.70       8.70  \n10    7.41    0.00    7.41       7.41  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>compression</th>\n      <th>novelty</th>\n      <th>redundancy</th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dialogue - Casual (in-domain)</td>\n      <td>0.657</td>\n      <td>0.200</td>\n      <td>0.0</td>\n      <td>52.63</td>\n      <td>22.22</td>\n      <td>42.11</td>\n      <td>42.11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Dialogue - Multi-speaker + emoji/noise</td>\n      <td>0.733</td>\n      <td>0.300</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Expository - General paragraph</td>\n      <td>0.489</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>21.05</td>\n      <td>5.56</td>\n      <td>15.79</td>\n      <td>15.79</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Expository - Technical explainer (long)</td>\n      <td>0.175</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>News - Short report</td>\n      <td>1.000</td>\n      <td>0.000</td>\n      <td>0.0</td>\n      <td>61.54</td>\n      <td>20.00</td>\n      <td>53.85</td>\n      <td>53.85</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Email - Work memo</td>\n      <td>0.767</td>\n      <td>0.048</td>\n      <td>0.0</td>\n      <td>58.54</td>\n      <td>15.38</td>\n      <td>34.15</td>\n      <td>34.15</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Meeting Notes - Bullets</td>\n      <td>0.679</td>\n      <td>0.471</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Noisy - Code-mixed + teencode</td>\n      <td>1.000</td>\n      <td>0.273</td>\n      <td>0.0</td>\n      <td>33.33</td>\n      <td>5.88</td>\n      <td>27.78</td>\n      <td>27.78</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Very Long - Truncation check</td>\n      <td>0.016</td>\n      <td>0.867</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Vietnamese - Expository</td>\n      <td>0.224</td>\n      <td>0.769</td>\n      <td>0.0</td>\n      <td>13.04</td>\n      <td>4.55</td>\n      <td>8.70</td>\n      <td>8.70</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Vietnamese - Dialogue (in-domain-ish)</td>\n      <td>0.423</td>\n      <td>0.600</td>\n      <td>0.0</td>\n      <td>7.41</td>\n      <td>0.00</td>\n      <td>7.41</td>\n      <td>7.41</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25}]}